{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfEddie/HypCLIP/blob/perceiver/lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fl7XFPNxUhyC",
        "outputId": "7be45eef-cc68-4b8d-8a30-ff5f981dd0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting salesforce-lavis\n",
            "  Downloading salesforce_lavis-1.0.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contexttimer (from salesforce-lavis)\n",
            "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting decord (from salesforce-lavis)\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.4.1 (from salesforce-lavis)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.4 (from salesforce-lavis)\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from salesforce-lavis)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iopath (from salesforce-lavis)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (7.34.0)\n",
            "Collecting omegaconf (from salesforce-lavis)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python-headless==4.5.5.64 (from salesforce-lavis)\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opendatasets (from salesforce-lavis)\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (1.5.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (5.15.0)\n",
            "Collecting pre-commit (from salesforce-lavis)\n",
            "  Downloading pre_commit-3.6.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycocoevalcap (from salesforce-lavis)\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.0.7)\n",
            "Collecting python-magic (from salesforce-lavis)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.19.3)\n",
            "Collecting sentencepiece (from salesforce-lavis)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (3.6.1)\n",
            "Collecting streamlit (from salesforce-lavis)\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12 (from salesforce-lavis)\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (4.66.1)\n",
            "Collecting transformers<4.27,>=4.25.0 (from salesforce-lavis)\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset (from salesforce-lavis)\n",
            "  Downloading webdataset-0.2.86-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from salesforce-lavis) (0.42.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless==4.5.5.64->salesforce-lavis) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->salesforce-lavis) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->salesforce-lavis) (0.2.12)\n",
            "Collecting portalocker (from iopath->salesforce-lavis)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->salesforce-lavis)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->salesforce-lavis) (4.9.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->salesforce-lavis)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets->salesforce-lavis) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets->salesforce-lavis) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->salesforce-lavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->salesforce-lavis) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->salesforce-lavis) (8.2.3)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->salesforce-lavis)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->salesforce-lavis)\n",
            "  Downloading identify-2.5.33-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->salesforce-lavis)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->salesforce-lavis)\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->salesforce-lavis) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (1.11.4)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->salesforce-lavis) (1.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (1.10.13)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->salesforce-lavis) (3.3.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->salesforce-lavis) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (5.3.2)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit->salesforce-lavis)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (10.0.1)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (13.7.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit->salesforce-lavis)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->salesforce-lavis)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->salesforce-lavis)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->salesforce-lavis) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit->salesforce-lavis)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting braceexpand (from webdataset->salesforce-lavis)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->salesforce-lavis) (3.17.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->salesforce-lavis) (0.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (3.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->salesforce-lavis) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->salesforce-lavis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->salesforce-lavis) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->salesforce-lavis) (3.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->salesforce-lavis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->salesforce-lavis) (0.1.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->salesforce-lavis)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (4.1.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets->salesforce-lavis) (8.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets->salesforce-lavis) (6.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->salesforce-lavis) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->salesforce-lavis) (0.1.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets->salesforce-lavis) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets->salesforce-lavis) (1.3)\n",
            "Building wheels for collected packages: fairscale, contexttimer, iopath, antlr4-python3-runtime\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292832 sha256=68a6c5a14c7e5604be8e7df1f6f9baa7b257771574ef4126c718eea1acea3744\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/58/6f/56c57fa8315eb0bcf0287b580c850845be5f116359b809e9f1\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=4327bc142ec8ea25e9c8b267cd7a759e07e06fb7def5e2cf8c287d9522e58245\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=e3ff198119a222a6d80ea5202a84315290fc3c818cebd933be648dc76ef33b79\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=0809aa84521c685664fe8b8932eb0aac37fe2e591fcbe62bf6134153f177ec81\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fairscale contexttimer iopath antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, sentencepiece, distlib, contexttimer, braceexpand, antlr4-python3-runtime, webdataset, watchdog, virtualenv, validators, smmap, python-magic, portalocker, opencv-python-headless, omegaconf, nodeenv, jedi, importlib-metadata, identify, ftfy, einops, decord, cfgv, pydeck, pre-commit, iopath, gitdb, transformers, opendatasets, gitpython, fairscale, timm, pycocoevalcap, streamlit, salesforce-lavis\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.1.78\n",
            "    Uninstalling opencv-python-headless-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.1.78\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 braceexpand-0.1.7 cfgv-3.4.0 contexttimer-0.3.3 decord-0.6.0 distlib-0.3.8 einops-0.7.0 fairscale-0.4.4 ftfy-6.1.3 gitdb-4.0.11 gitpython-3.1.40 identify-2.5.33 importlib-metadata-6.11.0 iopath-0.1.10 jedi-0.19.1 nodeenv-1.8.0 omegaconf-2.3.0 opencv-python-headless-4.5.5.64 opendatasets-0.1.22 portalocker-2.8.2 pre-commit-3.6.0 pycocoevalcap-1.2 pydeck-0.8.1b0 python-magic-0.4.27 salesforce-lavis-1.0.2 sentencepiece-0.1.99 smmap-5.0.1 streamlit-1.29.0 timm-0.4.12 tokenizers-0.13.3 transformers-4.26.1 validators-0.22.0 virtualenv-20.25.0 watchdog-3.0.0 webdataset-0.2.86\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install -U salesforce-lavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsjtNs6TCGQy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vrG_dS2SC-U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def dct(x, norm=None):\n",
        "    \"\"\"\n",
        "    Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
        "    :param x: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the DCT-II of the signal over the last dimension\n",
        "    \"\"\"\n",
        "\n",
        "    x_shape = x.shape\n",
        "    # print(x_shape)\n",
        "    N = x_shape[-1]\n",
        "    x = x.contiguous().view(-1, N)\n",
        "    # print('x', x.shape)\n",
        "    # print(x)\n",
        "\n",
        "    v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)\n",
        "    # print('v', v.shape)\n",
        "    # print(v)\n",
        "    Vc = torch.fft.fft(v, dim=1)\n",
        "    # print('vc', Vc.shape)\n",
        "    # print(Vc)\n",
        "\n",
        "    k = - torch.arange(N, dtype=x.dtype, device=x.device)[None, :] * np.pi / (2 * N)\n",
        "    # print('k', k.shape)\n",
        "    # print(k)\n",
        "    W_r = torch.cos(k)\n",
        "    W_i = torch.sin(k)\n",
        "\n",
        "    V = Vc.real * W_r - Vc.imag * W_i\n",
        "    # print('V', V.shape)\n",
        "    # print(V)\n",
        "\n",
        "    if norm == 'ortho':\n",
        "        V[:, 0] /= np.sqrt(N) * 2\n",
        "        V[:, 1:] /= np.sqrt(N / 2) * 2\n",
        "\n",
        "    V = 2 * V.view(*x_shape)\n",
        "    # print('V final', V.shape)\n",
        "    # print(V)\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "def idct(X, norm=None):\n",
        "    \"\"\"\n",
        "    The inverse to DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
        "    Our definition of idct is that idct(dct(x)) == x\n",
        "    For the meaning of the parameter `norm`, see:\n",
        "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
        "    :param X: the input signal\n",
        "    :param norm: the normalization, None or 'ortho'\n",
        "    :return: the inverse DCT-II of the signal over the last dimension\n",
        "    \"\"\"\n",
        "\n",
        "    x_shape = X.shape\n",
        "    N = x_shape[-1]\n",
        "\n",
        "    X_v = X.contiguous().view(-1, x_shape[-1]) / 2\n",
        "    if norm == 'ortho':\n",
        "        X_v[:, 0] *= np.sqrt(N) * 2\n",
        "        X_v[:, 1:] *= np.sqrt(N / 2) * 2\n",
        "\n",
        "    k = torch.arange(x_shape[-1], dtype=X.dtype, device=X.device)[None, :] * np.pi / (2 * N)\n",
        "    W_r = torch.cos(k)\n",
        "    W_i = torch.sin(k)\n",
        "\n",
        "    V_t_r = X_v\n",
        "    V_t_i = torch.cat([X_v[:, :1] * 0, -X_v.flip([1])[:, :-1]], dim=1)\n",
        "\n",
        "    V_r = V_t_r * W_r - V_t_i * W_i\n",
        "    V_i = V_t_r * W_i + V_t_i * W_r\n",
        "\n",
        "    V = torch.cat([V_r.unsqueeze(2), V_i.unsqueeze(2)], dim=2)\n",
        "    V = torch.view_as_complex(V)\n",
        "\n",
        "    v = torch.fft.ifft(V, dim=1).real\n",
        "    x = v.new_zeros(v.shape)\n",
        "    x[:, ::2] += v[:, :N - (N // 2)]\n",
        "    x[:, 1::2] += v.flip([1])[:, :N // 2]\n",
        "\n",
        "    return x.view(*x_shape)\n",
        "\n",
        "\n",
        "\n",
        "def dc_transform(x, r=0.8):\n",
        "    # cufft doesn't accept fp16\n",
        "    # dct along T dimension\n",
        "    print('original', x.shape)\n",
        "    x_dct = dct(x.transpose(0,2), norm='ortho').transpose(0,2)\n",
        "    print()\n",
        "    T, B, C = x_dct.size()\n",
        "    print('dct', x_dct.shape)\n",
        "\n",
        "    # feel free to play with any method here\n",
        "    x_dct = x_dct[:math.ceil(T* r), :, :]\n",
        "\n",
        "    return idct(x_dct.transpose(0,2), norm='ortho').transpose(0,2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plot_hidden_states(vision_output.hidden_states)\n",
        "\n",
        "# vision_output.hidden_states[-1][:,0,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpoIHkoLCMM3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "\n",
        "# model_ckt =  \"openai/clip-vit-large-patch14\"\n",
        "# model_ckt =  \"openai/clip-vit-base-patch16\"\n",
        "# model_ckt =  \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "from transformers import (\n",
        "    BlipPreTrainedModel,\n",
        "    BlipConfig,\n",
        "    BlipVisionModel,\n",
        "    BlipTextModel,\n",
        ")\n",
        "from transformers.models.blip.modeling_blip import BlipImageTextMatchingModelOutput\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class DCTBlipForImageTextRetrieval(BlipPreTrainedModel):\n",
        "    config_class = BlipConfig\n",
        "\n",
        "    def __init__(self, config: BlipConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = BlipVisionModel(config.vision_config)\n",
        "\n",
        "        self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
        "\n",
        "        self.vision_proj = nn.Linear(config.vision_config.hidden_size, config.image_text_hidden_size)\n",
        "\n",
        "        self.text_proj = nn.Linear(config.text_config.hidden_size, config.image_text_hidden_size)\n",
        "\n",
        "        self.itm_head = nn.Linear(config.text_config.hidden_size, 2)\n",
        "\n",
        "        self.decoder_pad_token_id = (\n",
        "            config.text_config.pad_token_id\n",
        "            if not hasattr(config, \"decoder_pad_token_id\")\n",
        "            else config.decoder_pad_token_id\n",
        "        )\n",
        "        self.decoder_start_token_id = (\n",
        "            config.text_config.bos_token_id\n",
        "            if not hasattr(config, \"decoder_start_token_id\")\n",
        "            else config.decoder_start_token_id\n",
        "        )\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self) -> nn.Module:\n",
        "        return self.vision_model.embeddings.patch_embedding\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        use_itm_head: Optional[bool] = True,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        vision_outputs = self.vision_model(\n",
        "            pixel_values=pixel_values,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        image_embeds = vision_outputs[0]\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
        "\n",
        "        if use_itm_head:\n",
        "            question_embeds = self.text_encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                encoder_hidden_states=image_embeds,\n",
        "                encoder_attention_mask=image_atts,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
        "\n",
        "            output = self.itm_head(question_embeds[:, 0, :])\n",
        "        else:\n",
        "            question_embeds = self.text_encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
        "\n",
        "            image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
        "            text_feat = F.normalize(self.text_proj(question_embeds[:, 0, :]), dim=-1)\n",
        "\n",
        "            output = image_feat @ text_feat.t()\n",
        "\n",
        "        if not return_dict:\n",
        "            outputs = (output, vision_outputs[0]) + vision_outputs[2:] + (question_embeds,)\n",
        "            return tuple(output for output in outputs if output is not None)\n",
        "\n",
        "        return BlipImageTextMatchingModelOutput(\n",
        "            itm_score=output,\n",
        "            last_hidden_state=vision_outputs.last_hidden_state,\n",
        "            hidden_states=vision_outputs.hidden_states,\n",
        "            attentions=vision_outputs.attentions,\n",
        "            question_embeds=question_embeds,\n",
        "        )\n",
        "\n",
        "    def get_vision_features(self,pixel_values):\n",
        "        state = self.vision_model.embeddings(pixel_values)\n",
        "        # state = self.vision_model.pre_layrnorm(state)\n",
        "        hidden_states = []\n",
        "        dct_signals = []\n",
        "        hidden_states.append(state)\n",
        "\n",
        "        for layer in self.vision_model.encoder.layers:\n",
        "            state = layer(state, None, None)[0]\n",
        "            dct_signals.append(dct(state[:,1:,:].permute(2,0,1)).transpose(0,2))\n",
        "\n",
        "            # cls = state[:, 0, :].unsqueeze(1)\n",
        "            # state = dc_transform(state[:,1:,:].permute(1,0,2), r=0.9).permute(1,0,2)\n",
        "            # state = torch.cat([cls, state], dim=1)\n",
        "            # state = dc_transform(state.permute(1,0,2)).permute(1,0,2)\n",
        "            hidden_states.append(state)\n",
        "\n",
        "        last_hidden_state = self.vision_model.post_layernorm(state)\n",
        "\n",
        "        pooled_output = last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.vision_model.post_layernorm(pooled_output)\n",
        "        return last_hidden_state, pooled_output, hidden_states, dct_signals\n",
        "\n",
        "    def get_text_features(self, input_ids, attention_mask):\n",
        "        question_embeds = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        question_embeds = question_embeds[0]\n",
        "        return  question_embeds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmDgJpZlCUu9",
        "outputId": "8c4ab60f-8661-452d-e9ec-2a7cfd370347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/blip-itm-base-flickr were not used when initializing BlipModel: ['text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_proj.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_proj.weight', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'vision_proj.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'itm_head.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'itm_head.bias', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.embeddings.position_ids', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'vision_proj.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight']\n",
            "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-base-flickr and are newly initialized: ['text_model.encoder.layer.10.attention.self.query.weight', 'logit_scale', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'visual_projection.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BlipModel\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_ckt =  \"Salesforce/blip-itm-base-flickr\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_ckt)\n",
        "model = BlipModel.from_pretrained(model_ckt).to(device)\n",
        "dct_model = DCTBlipForImageTextRetrieval.from_pretrained(model_ckt).to(device)\n",
        "\n",
        "vision_model = model.vision_model\n",
        "text_model = model.text_model\n",
        "\n",
        "# outputs = model(**inputs, output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAwPzObxVyAp",
        "outputId": "c4ec168f-622e-4586-bb4f-bae34fec7fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Two young guys with shaggy hair look at their hands while hanging out in the yard.', 'Two young, White males are outside near many bushes.', 'Two men in green shirts are standing in a yard.', 'A man in a blue shirt standing in a garden.', 'Two friends enjoy time spent together.'], ['Several men in hard hats are operating a giant pulley system.', 'Workers look down from up above on a piece of equipment.', 'Two men working on a machine wearing hard hats.', 'Four men on top of a tall structure.', 'Three men on a large rig.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "flickr = load_dataset(\"nlphuji/flickr30k\")['test']\n",
        "print(flickr[:2]['caption'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R9HkrcfWXIc"
      },
      "outputs": [],
      "source": [
        "inputs = processor(text=[\n",
        "    \"Two young guys with shaggy hair look at their hands while hanging out in the yard.\",\n",
        "    \"Two men in green shirts are standing in a yard\",\n",
        "    \"Two young, White males are outside near many bushes.\",\n",
        "    \"Workers look down from up above on a piece of equipment.\",\n",
        "    \"Four men on top of a tall structure\"\n",
        "],images=flickr[:2]['image'], return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga8nnPQbXMH-",
        "outputId": "db9062f3-5685-4743-a665-9876a17e8faf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0059, 0.0259, 0.0263, 0.0231, 0.0124],\n",
              "        [0.0146, 0.0241, 0.0317, 0.0280, 0.0317]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "vision_output = vision_model(inputs['pixel_values'].to(device), output_hidden_states=True)\n",
        "text_output = text_model(input_ids=inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device), output_hidden_states=True)\n",
        "\n",
        "vis_embed = F.normalize(model.visual_projection(vision_output[1]), dim=-1)\n",
        "\n",
        "text_embed = F.normalize(model.text_projection(text_output[1]), dim=-1)\n",
        "vis_embed @ text_embed.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmEy9aKM44JU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_hidden_states(hidden_states):\n",
        "  for hidden_state in hidden_states:\n",
        "    hidden_state=hidden_state.permute(1,0,2)\n",
        "    x_dct = dct(hidden_state.transpose(0,2), norm='ortho').transpose(0,2)\n",
        "    numpy_array = (torch.abs(x_dct.permute(1,0,2).mean(0).mean(1))**2).cpu().detach().numpy()\n",
        "    plt.figure(figsize=(10, 2))\n",
        "\n",
        "    # plt.imshow(numpy_array, cmap='viridis')  # You can choose a different colormap\n",
        "    plt.plot(numpy_array)# You can choose a different colormap\n",
        "    # plt.colorbar()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vision_output.hidden_states[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75iegUw4_nz0",
        "outputId": "6896f350-74a0-4be3-b0fd-65b9562588ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 577, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF9hwAxQqfij"
      },
      "outputs": [],
      "source": [
        "plot_hidden_states(vision_output.hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHa9YrhP4woQ"
      },
      "outputs": [],
      "source": [
        "from lavis import BlipRetrieval\n",
        "\n",
        "class DCTLAVISBlip(nn.Module):\n",
        "    config_class = BlipConfig\n",
        "\n",
        "    def __init__(self, model:BlipRetrieval):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vision_modejl = model.visual_encoder\n",
        "\n",
        "        self.text_encoder = model.text_encoder\n",
        "\n",
        "        self.vision_proj = model.vision_proj\n",
        "\n",
        "        self.text_proj = model.text_proj\n",
        "        self.r_list = nn.ParameterList([\n",
        "            1.0, 1.0, 1.0, 1.0, 1.0, 0.8,\n",
        "            0.8, 0.9, 0.9, 0.9, 1.0, 1.0,\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor=None,\n",
        "        pixel_values: torch.FloatTensor=None,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        apply_fourier: Optional[torch.LongTensor] = True,\n",
        "\n",
        "    ):\n",
        "        if input_ids is not None:\n",
        "            return self.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        else:\n",
        "            return self.get_vision_features(pixel_values=pixel_values, apply_fourier=apply_fourier)\n",
        "\n",
        "\n",
        "    def get_vision_features(self, pixel_values, apply_fourier=True):\n",
        "        B = pixel_values.shape[0]\n",
        "        hidden_states = []\n",
        "        x = self.vision_model.patch_embed(pixel_values)\n",
        "\n",
        "        cls_tokens = self.vision_model.cls_token.expand(\n",
        "            B, -1, -1\n",
        "        )\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.vision_model.pos_embed[:, : x.size(1), :]\n",
        "        x = self.vision_model.pos_drop(x)\n",
        "\n",
        "        for i, blk in enumerate(self.vision_model.blocks):\n",
        "            x = blk(x)\n",
        "            cls = x[:, 0, :].unsqueeze(1)\n",
        "            state = dc_transform(x[:,1:,:].permute(1,0,2), r=(self.r_list[i] if (self.training or apply_fourier) else 1.0)).permute(1,0,2)\n",
        "            x = torch.cat([cls, state], dim=1)\n",
        "            hidden_states.append(x)\n",
        "        x = self.vision_model.norm(x)\n",
        "\n",
        "        vision_embed = self.vision_proj(x[:,0,:])\n",
        "        return x, vision_embed\n",
        "\n",
        "    def get_text_features(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            class Text(object):\n",
        "                pass\n",
        "            text = Text()\n",
        "            text.input_ids=input_ids\n",
        "            text.attention_mask=attention_mask\n",
        "            question_embeds = self.text_encoder.forward_text(text)\n",
        "            last_hidden_state = question_embeds[0]\n",
        "            text_embed = self.text_proj(last_hidden_state[:,0,:])\n",
        "\n",
        "            return  last_hidden_state, text_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lavis.models import load_model_and_preprocess\n",
        "model, vis_processors, txt_processors = load_model_and_preprocess(\"blip_retrieval\", \"flickr\", is_eval=False)\n",
        "model = DCTLAVISBlip(model)"
      ],
      "metadata": {
        "id": "EomhLZ96FiFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60Mz3zqpFl5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNU3kY2hKMcYkBu4YiC5IrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}