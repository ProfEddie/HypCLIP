{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--use_graph USE_GRAPH] [--lr LR]\n",
      "                             [--dropout DROPOUT] [--cuda CUDA]\n",
      "                             [--epochs EPOCHS] [--weight_decay WEIGHT_DECAY]\n",
      "                             [--optimizer OPTIMIZER] [--momentum MOMENTUM]\n",
      "                             [--patience PATIENCE] [--seed SEED]\n",
      "                             [--log_freq LOG_FREQ] [--save SAVE]\n",
      "                             [--save_dir SAVE_DIR] [--sweep_c SWEEP_C]\n",
      "                             [--lr_reduce_freq LR_REDUCE_FREQ] [--gamma GAMMA]\n",
      "                             [--grad_clip GRAD_CLIP] [--min_epochs MIN_EPOCHS]\n",
      "                             [--mixed_precision MIXED_PRECISION]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--lorentz_pos_margin LORENTZ_POS_MARGIN]\n",
      "                             [--lorentz_neg_margin LORENTZ_NEG_MARGIN]\n",
      "                             [--lorentz_image_neg_margin LORENTZ_IMAGE_NEG_MARGIN]\n",
      "                             [--euclid_pos_margin EUCLID_POS_MARGIN]\n",
      "                             [--euclid_neg_margin EUCLID_NEG_MARGIN]\n",
      "                             [--max_txt_len MAX_TXT_LEN]\n",
      "                             [--negative_all_rank NEGATIVE_ALL_RANK]\n",
      "                             [--alpha ALPHA] [--queue_size QUEUE_SIZE]\n",
      "                             [--batch_size BATCH_SIZE] [--eval_freq EVAL_FREQ]\n",
      "                             [--weight_i2t WEIGHT_I2T]\n",
      "                             [--enable_log ENABLE_LOG]\n",
      "                             [--use_margin_loss USE_MARGIN_LOSS]\n",
      "                             [--use_graph_loss USE_GRAPH_LOSS]\n",
      "                             [--use_entailment_loss USE_ENTAILMENT_LOSS]\n",
      "                             [--hyp_margin_loss_weight HYP_MARGIN_LOSS_WEIGHT]\n",
      "                             [--num_proj_layers NUM_PROJ_LAYERS]\n",
      "                             [--proj_layer_hidden_sizes PROJ_LAYER_HIDDEN_SIZES]\n",
      "                             [--normalize_text_embed NORMALIZE_TEXT_EMBED]\n",
      "                             [--normalize_image_embed NORMALIZE_IMAGE_EMBED]\n",
      "                             [--shared_proj_layers SHARED_PROJ_LAYERS]\n",
      "                             [--use_itm_head USE_ITM_HEAD]\n",
      "                             [--use_root USE_ROOT]\n",
      "                             [--graph_hidden_channels GRAPH_HIDDEN_CHANNELS]\n",
      "                             [--model_ckt MODEL_CKT] [--manifold MANIFOLD]\n",
      "                             [--curv CURV] [--atol ATOL] [--rtol RTOL]\n",
      "                             [--temp TEMP] [--clip_radius CLIP_RADIUS]\n",
      "                             [--vision_trainable_blocks VISION_TRAINABLE_BLOCKS]\n",
      "                             [--text_trainable_blocks TEXT_TRAINABLE_BLOCKS]\n",
      "                             [--num_vision_hidden_states NUM_VISION_HIDDEN_STATES]\n",
      "                             [--num_text_hidden_states NUM_TEXT_HIDDEN_STATES]\n",
      "                             [--ft_out FT_OUT]\n",
      "                             [--curv_learnable CURV_LEARNABLE]\n",
      "                             [--freeze_embedding FREEZE_EMBEDDING]\n",
      "                             [--use_lorentz_centroid USE_LORENTZ_CENTROID]\n",
      "                             [--fourier FOURIER]\n",
      "                             [--soft_target_loss SOFT_TARGET_LOSS]\n",
      "                             [--dataset DATASET] [--cache_dir CACHE_DIR]\n",
      "                             [--num_latents NUM_LATENTS]\n",
      "                             [--d_latents D_LATENTS] [--num_blocks NUM_BLOCKS]\n",
      "                             [--num_self_attends_per_block NUM_SELF_ATTENDS_PER_BLOCK]\n",
      "                             [--num_cross_attention_heads NUM_CROSS_ATTENTION_HEADS]\n",
      "                             [--num_self_attention_heads NUM_SELF_ATTENTION_HEADS]\n",
      "                             [--cross_attention_widening_factor CROSS_ATTENTION_WIDENING_FACTOR]\n",
      "                             [--attention_probs_dropout_prob ATTENTION_PROBS_DROPOUT_PROB]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/nmduy/.local/share/jupyter/runtime/kernel-v2-324297sC2aX2Fr7Xqi.json could match --ft_out, --freeze_embedding, --fourier\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarvis/anaconda3/envs/hada-v2/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from model.modules.fuseModel import FuseEncoder \n",
    "from lavis.datasets.builders import load_dataset\n",
    "from model.modules.fuseModel import LavisEncoder, CLIPEncoder \n",
    "from utils.data_utils import get_fused_dataloader \n",
    "from lavis.models import load_model_and_preprocess\n",
    "from hyptorch.lorentz.manifold import CustomLorentz\n",
    "from transformers import AutoProcessor \n",
    "from config import parser\n",
    "from typing import List\n",
    "from config import CLIP_BASE_PATCH_16, CLIP_BASE_PATCH_32, COCO_PATH, FLICKR_PATH \n",
    "\n",
    "config = parser.parse_args()\n",
    "manifold = CustomLorentz()\n",
    "\n",
    "dataset = load_dataset(\"flickr30k\", vis_path=FLICKR_PATH, cfg_path=None)\n",
    "model_ckts = [CLIP_BASE_PATCH_16, CLIP_BASE_PATCH_32]\n",
    "\n",
    "def prepare_processors(model_ckts:List[str]):\n",
    "    tokenizers = []\n",
    "    vis_processors = []\n",
    "    txt_processors = []\n",
    "    for model_ckt in model_ckts:\n",
    "        if 'lavis' not in model_ckt:\n",
    "            model, vis_processor, txt_processor = load_model_and_preprocess(\"blip_retrieval\", \"coco\", is_eval=False)\n",
    "            txt_processors.append(model.tokenizer)\n",
    "            vis_processors.append(vis_processor['eval'])\n",
    "            tokenizers.append(txt_processor['eval'])\n",
    "        else:\n",
    "            txt_processors.append(None)\n",
    "            vis_processors.append(AutoProcessor.from_pretrained(model_ckt))\n",
    "            tokenizers.append(AutoProcessor.from_pretrained(model_ckt))\n",
    "    return tokenizers, vis_processors, txt_processors\n",
    "\n",
    "\n",
    "tokenizers, vis_processors, txt_processor = prepare_processors(model_ckts)\n",
    "        \n",
    "\n",
    "loaders = get_fused_dataloader(dataset, vis_processors=vis_processors, tokenizers=tokenizers, txt_processors=txt_processors, batch_size=40) \n",
    "\n",
    "# model = FuseEncoder(\n",
    "#     config,           \n",
    "#     d_visions=[100, 100], \n",
    "#     d_texts=[100, 100], \n",
    "#     ft_out=256, \n",
    "#     vision_bodies=[], \n",
    "#     text_bodies=[],\n",
    "#     vision_head=None,\n",
    "#     text_head=None,\n",
    "#     mapper=None,\n",
    "#     manifold=None, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 12, 32])\n",
      "torch.Size([40, 40, 12, 32])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_t2q = torch.matmul(\n",
    "    a, b.permute(0, 2, 1)\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 40])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_t2q.max(-1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hada-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
